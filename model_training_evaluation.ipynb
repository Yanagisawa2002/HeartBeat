{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ«€ ECGå¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°\n",
    "\n",
    "æœ¬notebookåŒ…å«å››ç§æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°ï¼š\n",
    "- CNN1D\n",
    "- LSTM\n",
    "- ResNet1D\n",
    "- Hybrid CNN-LSTM\n",
    "\n",
    "æ¯ä¸ªæ¨¡å‹éƒ½æœ‰ç‹¬ç«‹çš„è®­ç»ƒä»£ç å—ï¼Œæœ€åè¿›è¡Œç»¼åˆè¯„ä¼°å’Œå¯è§†åŒ–å¯¹æ¯”ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ å¯¼å…¥å¿…è¦çš„åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—\n",
    "from src.comparison_models import create_comparison_model\n",
    "from src.data_loader import PTBDataLoader\n",
    "from src.model_comparison import ModelComparison\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# è®¾ç½®è®¾å¤‡\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'ä½¿ç”¨è®¾å¤‡: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š æ•°æ®åŠ è½½å’Œé¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½æ•°æ®\n",
    "print(\"åŠ è½½ECGæ•°æ®...\")\n",
    "\n",
    "# åŠ è½½é¢„å¤„ç†åçš„æ•°æ®\n",
    "X_train = np.load('data/processed/X_train.npy')\n",
    "X_val = np.load('data/processed/X_val.npy')\n",
    "X_test = np.load('data/processed/X_test.npy')\n",
    "y_train = np.load('data/processed/y_train.npy')\n",
    "y_val = np.load('data/processed/y_val.npy')\n",
    "y_test = np.load('data/processed/y_test.npy')\n",
    "\n",
    "print(f'è®­ç»ƒé›†å½¢çŠ¶: {X_train.shape}, æ ‡ç­¾: {y_train.shape}')\n",
    "print(f'éªŒè¯é›†å½¢çŠ¶: {X_val.shape}, æ ‡ç­¾: {y_val.shape}')\n",
    "print(f'æµ‹è¯•é›†å½¢çŠ¶: {X_test.shape}, æ ‡ç­¾: {y_test.shape}')\n",
    "\n",
    "# è½¬æ¢ä¸ºPyTorchå¼ é‡\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train).to(device)\n",
    "y_val_tensor = torch.LongTensor(y_val).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test).to(device)\n",
    "\n",
    "print('æ•°æ®åŠ è½½å®Œæˆï¼')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ è®­ç»ƒå‡½æ•°å®šä¹‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, model_name, X_train, y_train, X_val, y_val, epochs=50, lr=0.001, batch_size=32):\n",
    "    \"\"\"è®­ç»ƒæ¨¡å‹çš„é€šç”¨å‡½æ•°\"\"\"\n",
    "    print(f'\\nå¼€å§‹è®­ç»ƒ {model_name} æ¨¡å‹...')\n",
    "    \n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # è®­ç»ƒé˜¶æ®µ\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        \n",
    "        # æ‰¹é‡è®­ç»ƒ\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            batch_X = X_train[i:i+batch_size]\n",
    "            batch_y = y_train[i:i+batch_size]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_correct += (predicted == batch_y).sum().item()\n",
    "        \n",
    "        # éªŒè¯é˜¶æ®µ\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val)\n",
    "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "            val_correct = (val_predicted == y_val).sum().item()\n",
    "        \n",
    "        # è®¡ç®—å‡†ç¡®ç‡\n",
    "        train_acc = train_correct / len(X_train)\n",
    "        val_acc = val_correct / len(X_val)\n",
    "        \n",
    "        train_losses.append(train_loss / (len(X_train) // batch_size))\n",
    "        val_losses.append(val_loss.item())\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/(len(X_train)//batch_size):.4f}, '\n",
    "                  f'Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f'{model_name} è®­ç»ƒå®Œæˆï¼è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’')\n",
    "    \n",
    "    return model, {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, model_name, X_test, y_test):\n",
    "    \"\"\"è¯„ä¼°æ¨¡å‹æ€§èƒ½\"\"\"\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # è½¬æ¢ä¸ºnumpyæ•°ç»„è¿›è¡Œè¯„ä¼°\n",
    "    y_true = y_test.cpu().numpy()\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_prob = probabilities[:, 1].cpu().numpy()  # å¼‚å¸¸ç±»åˆ«çš„æ¦‚ç‡\n",
    "    \n",
    "    # è®¡ç®—å„ç§æŒ‡æ ‡\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    \n",
    "    # è®¡ç®—å‚æ•°æ•°é‡\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'AUC Score': auc,\n",
    "        'Parameters': total_params,\n",
    "        'Inference Time (s)': inference_time\n",
    "    }\n",
    "    \n",
    "    print(f'\\n{model_name} è¯„ä¼°ç»“æœ:')\n",
    "    for key, value in results.items():\n",
    "        if key != 'Model':\n",
    "            if 'Time' in key or key == 'Parameters':\n",
    "                print(f'{key}: {value:.0f}' if key == 'Parameters' else f'{key}: {value:.4f}')\n",
    "            else:\n",
    "                print(f'{key}: {value:.4f}')\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  æ¨¡å‹1: CNN1Dè®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºCNN1Dæ¨¡å‹\n",
    "print('=' * 60)\n",
    "print('è®­ç»ƒ CNN1D æ¨¡å‹')\n",
    "print('=' * 60)\n",
    "\n",
    "cnn1d_model = create_comparison_model('cnn1d', input_size=X_train.shape[1])\n",
    "print(f'CNN1Dæ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in cnn1d_model.parameters()):,}')\n",
    "\n",
    "# è®­ç»ƒCNN1Dæ¨¡å‹\n",
    "cnn1d_trained, cnn1d_history = train_model(\n",
    "    cnn1d_model, 'CNN1D', \n",
    "    X_train_tensor, y_train_tensor, \n",
    "    X_val_tensor, y_val_tensor,\n",
    "    epochs=50, lr=0.001, batch_size=32\n",
    ")\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹\n",
    "torch.save(cnn1d_trained.state_dict(), 'models/cnn1d_model.pth')\n",
    "print('CNN1Dæ¨¡å‹å·²ä¿å­˜åˆ° models/cnn1d_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ æ¨¡å‹2: LSTMè®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºLSTMæ¨¡å‹\n",
    "print('=' * 60)\n",
    "print('è®­ç»ƒ LSTM æ¨¡å‹')\n",
    "print('=' * 60)\n",
    "\n",
    "lstm_model = create_comparison_model('lstm', input_size=X_train.shape[1])\n",
    "print(f'LSTMæ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in lstm_model.parameters()):,}')\n",
    "\n",
    "# è®­ç»ƒLSTMæ¨¡å‹\n",
    "lstm_trained, lstm_history = train_model(\n",
    "    lstm_model, 'LSTM', \n",
    "    X_train_tensor, y_train_tensor, \n",
    "    X_val_tensor, y_val_tensor,\n",
    "    epochs=50, lr=0.001, batch_size=32\n",
    ")\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹\n",
    "torch.save(lstm_trained.state_dict(), 'models/lstm_model.pth')\n",
    "print('LSTMæ¨¡å‹å·²ä¿å­˜åˆ° models/lstm_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ æ¨¡å‹3: ResNet1Dè®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºResNet1Dæ¨¡å‹\n",
    "print('=' * 60)\n",
    "print('è®­ç»ƒ ResNet1D æ¨¡å‹')\n",
    "print('=' * 60)\n",
    "\n",
    "resnet1d_model = create_comparison_model('resnet1d', input_size=X_train.shape[1])\n",
    "print(f'ResNet1Dæ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in resnet1d_model.parameters()):,}')\n",
    "\n",
    "# è®­ç»ƒResNet1Dæ¨¡å‹\n",
    "resnet1d_trained, resnet1d_history = train_model(\n",
    "    resnet1d_model, 'ResNet1D', \n",
    "    X_train_tensor, y_train_tensor, \n",
    "    X_val_tensor, y_val_tensor,\n",
    "    epochs=50, lr=0.001, batch_size=32\n",
    ")\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹\n",
    "torch.save(resnet1d_trained.state_dict(), 'models/resnet1d_model.pth')\n",
    "print('ResNet1Dæ¨¡å‹å·²ä¿å­˜åˆ° models/resnet1d_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”€ æ¨¡å‹4: Hybrid CNN-LSTMè®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºHybrid CNN-LSTMæ¨¡å‹\n",
    "print('=' * 60)\n",
    "print('è®­ç»ƒ Hybrid CNN-LSTM æ¨¡å‹')\n",
    "print('=' * 60)\n",
    "\n",
    "hybrid_model = create_comparison_model('hybrid_cnn_lstm', input_size=X_train.shape[1])\n",
    "print(f'Hybrid CNN-LSTMæ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in hybrid_model.parameters()):,}')\n",
    "\n",
    "# è®­ç»ƒHybrid CNN-LSTMæ¨¡å‹\n",
    "hybrid_trained, hybrid_history = train_model(\n",
    "    hybrid_model, 'Hybrid CNN-LSTM', \n",
    "    X_train_tensor, y_train_tensor, \n",
    "    X_val_tensor, y_val_tensor,\n",
    "    epochs=50, lr=0.001, batch_size=32\n",
    ")\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹\n",
    "torch.save(hybrid_trained.state_dict(), 'models/hybrid_cnn_lstm_model.pth')\n",
    "print('Hybrid CNN-LSTMæ¨¡å‹å·²ä¿å­˜åˆ° models/hybrid_cnn_lstm_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š æ¨¡å‹è¯„ä¼°å’Œå¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('æ¨¡å‹è¯„ä¼°å’Œå¯¹æ¯”')\n",
    "print('=' * 80)\n",
    "\n",
    "# è¯„ä¼°æ‰€æœ‰æ¨¡å‹\n",
    "models = [\n",
    "    (cnn1d_trained, 'CNN1D', cnn1d_history),\n",
    "    (lstm_trained, 'LSTM', lstm_history),\n",
    "    (resnet1d_trained, 'ResNet1D', resnet1d_history),\n",
    "    (hybrid_trained, 'Hybrid CNN-LSTM', hybrid_history)\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for model, name, history in models:\n",
    "    result = evaluate_model(model, name, X_test_tensor, y_test_tensor)\n",
    "    result['Training Time (s)'] = history['training_time']\n",
    "    results.append(result)\n",
    "\n",
    "# åˆ›å»ºç»“æœDataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print('\\n' + '=' * 80)\n",
    "print('æ‰€æœ‰æ¨¡å‹è¯„ä¼°ç»“æœæ±‡æ€»')\n",
    "print('=' * 80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "results_df.to_csv('results/comparison/model_comparison_results.csv', index=False)\n",
    "print('\\nè¯„ä¼°ç»“æœå·²ä¿å­˜åˆ° results/comparison/model_comparison_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ è®­ç»ƒå†å²å¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»˜åˆ¶è®­ç»ƒå†å²\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('æ¨¡å‹è®­ç»ƒå†å²å¯¹æ¯”', fontsize=16, fontweight='bold')\n",
    "\n",
    "histories = [cnn1d_history, lstm_history, resnet1d_history, hybrid_history]\n",
    "model_names = ['CNN1D', 'LSTM', 'ResNet1D', 'Hybrid CNN-LSTM']\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "\n",
    "# è®­ç»ƒæŸå¤±\n",
    "for i, (history, name, color) in enumerate(zip(histories, model_names, colors)):\n",
    "    axes[0, 0].plot(history['train_losses'], label=f'{name}', color=color)\n",
    "axes[0, 0].set_title('è®­ç»ƒæŸå¤±')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# éªŒè¯æŸå¤±\n",
    "for i, (history, name, color) in enumerate(zip(histories, model_names, colors)):\n",
    "    axes[0, 1].plot(history['val_losses'], label=f'{name}', color=color)\n",
    "axes[0, 1].set_title('éªŒè¯æŸå¤±')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# è®­ç»ƒå‡†ç¡®ç‡\n",
    "for i, (history, name, color) in enumerate(zip(histories, model_names, colors)):\n",
    "    axes[1, 0].plot(history['train_accuracies'], label=f'{name}', color=color)\n",
    "axes[1, 0].set_title('è®­ç»ƒå‡†ç¡®ç‡')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# éªŒè¯å‡†ç¡®ç‡\n",
    "for i, (history, name, color) in enumerate(zip(histories, model_names, colors)):\n",
    "    axes[1, 1].plot(history['val_accuracies'], label=f'{name}', color=color)\n",
    "axes[1, 1].set_title('éªŒè¯å‡†ç¡®ç‡')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Accuracy')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/training_history_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('è®­ç»ƒå†å²å›¾è¡¨å·²ä¿å­˜åˆ° results/training_history_comparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ æ€§èƒ½å¯¹æ¯”å¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('æ¨¡å‹æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['Accuracy', 'F1 Score', 'AUC Score', 'Training Time (s)']\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i//2, i%2]\n",
    "    values = results_df[metric].values\n",
    "    bars = ax.bar(results_df['Model'], values, color=colors[i], alpha=0.8)\n",
    "    ax.set_title(f'{metric} å¯¹æ¯”')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        if metric == 'Training Time (s)':\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                   f'{value:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
    "        else:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                   f'{value:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('æ€§èƒ½å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜åˆ° results/performance_comparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” æ··æ·†çŸ©é˜µå¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸ºæ¯ä¸ªæ¨¡å‹ç”Ÿæˆæ··æ·†çŸ©é˜µ\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('æ¨¡å‹æ··æ·†çŸ©é˜µå¯¹æ¯”', fontsize=16, fontweight='bold')\n",
    "\n",
    "y_true = y_test_tensor.cpu().numpy()\n",
    "\n",
    "for i, (model, name, _) in enumerate(models):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    ax = axes[i//2, i%2]\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['æ­£å¸¸', 'å¼‚å¸¸'], yticklabels=['æ­£å¸¸', 'å¼‚å¸¸'])\n",
    "    ax.set_title(f'{name} æ··æ·†çŸ©é˜µ')\n",
    "    ax.set_xlabel('é¢„æµ‹æ ‡ç­¾')\n",
    "    ax.set_ylabel('çœŸå®æ ‡ç­¾')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('æ··æ·†çŸ©é˜µå›¾è¡¨å·²ä¿å­˜åˆ° results/confusion_matrices.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ è¿è¡Œå®Œæ•´çš„å¯è§†åŒ–è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿è¡Œå®Œæ•´çš„å¯è§†åŒ–è¯„ä¼°è„šæœ¬\n",
    "print('=' * 80)\n",
    "print('è¿è¡Œå®Œæ•´çš„å¯è§†åŒ–è¯„ä¼°')\n",
    "print('=' * 80)\n",
    "\n",
    "# è¿è¡Œå¯è§†åŒ–è„šæœ¬\n",
    "exec(open('visualize_all_models.py').read())\n",
    "\n",
    "print('\\nå®Œæ•´çš„å¯è§†åŒ–è¯„ä¼°å·²å®Œæˆï¼')\n",
    "print('è¯·æŸ¥çœ‹ä»¥ä¸‹æ–‡ä»¶å¤¹ä¸­çš„ç»“æœ:')\n",
    "print('- results/visualization/ - æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨')\n",
    "print('- view_results.html - æœ¬åœ°HTMLå±•ç¤ºé¡µé¢')\n",
    "print('- EVALUATION_RESULTS.md - è¯¦ç»†è¯„ä¼°ç»“æœæ–‡æ¡£')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ æ€»ç»“å’Œå»ºè®®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆæ€»ç»“æŠ¥å‘Š\n",
    "print('=' * 80)\n",
    "print('æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°æ€»ç»“')\n",
    "print('=' * 80)\n",
    "\n",
    "# æ‰¾å‡ºæœ€ä½³æ¨¡å‹\n",
    "best_accuracy = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "best_f1 = results_df.loc[results_df['F1 Score'].idxmax()]\n",
    "best_auc = results_df.loc[results_df['AUC Score'].idxmax()]\n",
    "fastest_training = results_df.loc[results_df['Training Time (s)'].idxmin()]\n",
    "fastest_inference = results_df.loc[results_df['Inference Time (s)'].idxmin()]\n",
    "\n",
    "print('ğŸ† æ€§èƒ½é¢†å…ˆè€…:')\n",
    "print(f'æœ€é«˜å‡†ç¡®ç‡: {best_accuracy[\"Model\"]} ({best_accuracy[\"Accuracy\"]:.4f})')\n",
    "print(f'æœ€é«˜F1åˆ†æ•°: {best_f1[\"Model\"]} ({best_f1[\"F1 Score\"]:.4f})')\n",
    "print(f'æœ€é«˜AUCåˆ†æ•°: {best_auc[\"Model\"]} ({best_auc[\"AUC Score\"]:.4f})')\n",
    "print(f'æœ€å¿«è®­ç»ƒ: {fastest_training[\"Model\"]} ({fastest_training[\"Training Time (s)\"]:.2f}ç§’)')\n",
    "print(f'æœ€å¿«æ¨ç†: {fastest_inference[\"Model\"]} ({fastest_inference[\"Inference Time (s)\"]:.4f}ç§’)')\n",
    "\n",
    "print('\\nğŸ¯ ä½¿ç”¨å»ºè®®:')\n",
    "print('â€¢ è¿½æ±‚æœ€é«˜å‡†ç¡®ç‡: ä½¿ç”¨ LSTM æ¨¡å‹')\n",
    "print('â€¢ å¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡: ä½¿ç”¨ CNN1D æ¨¡å‹')\n",
    "print('â€¢ å¿«é€Ÿè®­ç»ƒéœ€æ±‚: ä½¿ç”¨ Hybrid CNN-LSTM æ¨¡å‹')\n",
    "print('â€¢ å®æ—¶æ¨ç†åº”ç”¨: ä½¿ç”¨ CNN1D æˆ– Hybrid CNN-LSTM æ¨¡å‹')\n",
    "\n",
    "print('âœ… æ‰€æœ‰æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å·²å®Œæˆï¼')\n",
    "print('ğŸ“Š å¯è§†åŒ–ç»“æœå·²ç”Ÿæˆï¼Œå¯é€šè¿‡ view_results.html æŸ¥çœ‹è¯¦ç»†å¯¹æ¯”')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}