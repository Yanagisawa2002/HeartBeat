#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´æ¨¡å‹è¯„ä¼°è„šæœ¬
å¯¹æ‰€æœ‰è®­ç»ƒäº†100ä¸ªepochçš„æ¨¡å‹è¿›è¡Œå…¨é¢è¯„ä¼°
"""

import os
import sys
import time
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
    confusion_matrix, classification_report, roc_curve, precision_recall_curve
)
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append('src')
from src.comparison_models import create_comparison_model


# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

def load_test_data():
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    print("Loading test data...")
    
    X_test = np.load('data/processed/X_test.npy')
    y_test = np.load('data/processed/y_test.npy')
    
    print(f"Test data shape: {X_test.shape}")
    print(f"Test labels shape: {y_test.shape}")
    print(f"Class distribution: {np.bincount(y_test)}")
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X_test = torch.FloatTensor(X_test)
    y_test = torch.LongTensor(y_test)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    test_dataset = TensorDataset(X_test, y_test)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    return test_loader, X_test, y_test

def evaluate_model_comprehensive(model, test_loader, device, model_name):
    """å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    print(f"\nEvaluating {model_name}...")
    
    model.eval()
    all_preds = []
    all_labels = []
    all_probs = []
    inference_times = []
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            
            # æµ‹é‡æ¨ç†æ—¶é—´
            start_time = time.time()
            output = model(data)
            inference_time = time.time() - start_time
            inference_times.append(inference_time)
            
            # è·å–é¢„æµ‹å’Œæ¦‚ç‡
            probs = torch.softmax(output, dim=1)
            preds = torch.argmax(output, dim=1)
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(target.cpu().numpy())
            all_probs.extend(probs[:, 1].cpu().numpy())  # å¼‚å¸¸ç±»çš„æ¦‚ç‡
    
    # è®¡ç®—å„ç§æŒ‡æ ‡
    accuracy = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)
    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)
    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)
    auc = roc_auc_score(all_labels, all_probs)
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
    precision_per_class = precision_score(all_labels, all_preds, average=None, zero_division=0)
    recall_per_class = recall_score(all_labels, all_preds, average=None, zero_division=0)
    f1_per_class = f1_score(all_labels, all_preds, average=None, zero_division=0)
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(all_labels, all_preds)
    
    # ROCæ›²çº¿æ•°æ®
    fpr, tpr, _ = roc_curve(all_labels, all_probs)
    
    # PRæ›²çº¿æ•°æ®
    precision_curve, recall_curve, _ = precision_recall_curve(all_labels, all_probs)
    
    # æ€»æ¨ç†æ—¶é—´
    total_inference_time = sum(inference_times)
    
    results = {
        'model_name': model_name,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'auc': auc,
        'precision_per_class': precision_per_class,
        'recall_per_class': recall_per_class,
        'f1_per_class': f1_per_class,
        'confusion_matrix': cm,
        'fpr': fpr,
        'tpr': tpr,
        'precision_curve': precision_curve,
        'recall_curve': recall_curve,
        'inference_time': total_inference_time,
        'predictions': all_preds,
        'probabilities': all_probs,
        'true_labels': all_labels
    }
    
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall: {recall:.4f}")
    print(f"  F1 Score: {f1:.4f}")
    print(f"  AUC Score: {auc:.4f}")
    print(f"  Inference Time: {total_inference_time:.4f}s")
    
    return results

def load_and_evaluate_model(model_path, model_type, test_loader, device):
    """åŠ è½½å¹¶è¯„ä¼°æŒ‡å®šæ¨¡å‹"""
    if not os.path.exists(model_path):
        print(f"Model file not found: {model_path}")
        return None
    
    try:
        # åˆ›å»ºæ¨¡å‹
        model = create_comparison_model(model_type, input_dim=12, seq_len=500, num_classes=2)
        
        # åŠ è½½æ¨¡å‹æƒé‡
        checkpoint = torch.load(model_path, map_location=device)
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        
        model = model.to(device)
        
        # è¯„ä¼°æ¨¡å‹
        results = evaluate_model_comprehensive(model, test_loader, device, model_type)
        
        # æ·»åŠ å‚æ•°æ•°é‡
        results['parameters'] = sum(p.numel() for p in model.parameters())
        
        return results
        
    except Exception as e:
        print(f"Error loading/evaluating {model_type}: {e}")
        return None

def create_detailed_report(all_results):
    """åˆ›å»ºè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š"""
    print("\n" + "="*80)
    print("è¯¦ç»†æ¨¡å‹è¯„ä¼°æŠ¥å‘Š")
    print("="*80)
    print(f"è¯„ä¼°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"è¯„ä¼°æ¨¡å‹æ•°é‡: {len(all_results)}")
    
    # æŒ‰AUCåˆ†æ•°æ’åº
    sorted_results = sorted(all_results, key=lambda x: x['auc'], reverse=True)
    
    print("\nğŸ“Š æ¨¡å‹æ€§èƒ½æ’å (æŒ‰AUCåˆ†æ•°):")
    print("-" * 80)
    
    for i, result in enumerate(sorted_results, 1):
        print(f"\n{i}. {result['model_name']}")
        print(f"   å‡†ç¡®ç‡: {result['accuracy']:.4f}")
        print(f"   ç²¾ç¡®ç‡: {result['precision']:.4f}")
        print(f"   å¬å›ç‡: {result['recall']:.4f}")
        print(f"   F1åˆ†æ•°: {result['f1']:.4f}")
        print(f"   AUCåˆ†æ•°: {result['auc']:.4f}")
        print(f"   æ¨ç†æ—¶é—´: {result['inference_time']:.4f}ç§’")
        print(f"   å‚æ•°æ•°é‡: {result['parameters']:,}")
        
        # æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡
        print(f"   æ­£å¸¸ç±» - ç²¾ç¡®ç‡: {result['precision_per_class'][0]:.4f}, å¬å›ç‡: {result['recall_per_class'][0]:.4f}, F1: {result['f1_per_class'][0]:.4f}")
        print(f"   å¼‚å¸¸ç±» - ç²¾ç¡®ç‡: {result['precision_per_class'][1]:.4f}, å¬å›ç‡: {result['recall_per_class'][1]:.4f}, F1: {result['f1_per_class'][1]:.4f}")
    
    # æ€§èƒ½å¯¹æ¯”åˆ†æ
    print("\nğŸ” æ€§èƒ½åˆ†æ:")
    print("-" * 50)
    
    best_accuracy = max(all_results, key=lambda x: x['accuracy'])
    best_auc = max(all_results, key=lambda x: x['auc'])
    fastest_inference = min(all_results, key=lambda x: x['inference_time'])
    most_efficient = min(all_results, key=lambda x: x['parameters'])
    
    print(f"â€¢ æœ€é«˜å‡†ç¡®ç‡: {best_accuracy['model_name']} ({best_accuracy['accuracy']:.4f})")
    print(f"â€¢ æœ€é«˜AUCåˆ†æ•°: {best_auc['model_name']} ({best_auc['auc']:.4f})")
    print(f"â€¢ æœ€å¿«æ¨ç†: {fastest_inference['model_name']} ({fastest_inference['inference_time']:.4f}ç§’)")
    print(f"â€¢ æœ€å°‘å‚æ•°: {most_efficient['model_name']} ({most_efficient['parameters']:,}ä¸ª)")
    
    return sorted_results

def save_comprehensive_results(all_results):
    """ä¿å­˜å®Œæ•´çš„è¯„ä¼°ç»“æœ"""
    # æ›´æ–°CSVæ–‡ä»¶
    csv_data = []
    for result in all_results:
        csv_data.append({
            'Model': result['model_name'],
            'Accuracy': result['accuracy'],
            'Precision': result['precision'],
            'Recall': result['recall'],
            'F1 Score': result['f1'],
            'AUC Score': result['auc'],
            'Training Time (s)': 3000.0,  # ä¼°è®¡å€¼ï¼Œå®é™…è®­ç»ƒæ—¶é—´å¯èƒ½ä¸åŒ
            'Inference Time (s)': result['inference_time'],
            'Parameters': result['parameters']
        })
    
    df = pd.DataFrame(csv_data)
    csv_path = 'results/comprehensive_model_evaluation.csv'
    df.to_csv(csv_path, index=False)
    print(f"\nğŸ’¾ å®Œæ•´è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {csv_path}")
    
    # ä¿å­˜è¯¦ç»†ç»“æœåˆ°pickleæ–‡ä»¶
    import pickle
    pickle_path = 'results/detailed_evaluation_results.pkl'
    with open(pickle_path, 'wb') as f:
        pickle.dump(all_results, f)
    print(f"ğŸ’¾ è¯¦ç»†è¯„ä¼°æ•°æ®å·²ä¿å­˜åˆ°: {pickle_path}")

def create_comparison_visualizations(all_results):
    """åˆ›å»ºå¯¹æ¯”å¯è§†åŒ–å›¾è¡¨"""
    print("\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    
    # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
    os.makedirs('results/comprehensive_evaluation', exist_ok=True)
    
    # 1. æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('æ¨¡å‹æ€§èƒ½å…¨é¢å¯¹æ¯”', fontsize=16, fontweight='bold')
    
    models = [r['model_name'] for r in all_results]
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'][:len(models)]
    
    # å‡†ç¡®ç‡
    accuracies = [r['accuracy'] for r in all_results]
    axes[0,0].bar(models, accuracies, color=colors)
    axes[0,0].set_title('å‡†ç¡®ç‡å¯¹æ¯”')
    axes[0,0].set_ylabel('å‡†ç¡®ç‡')
    axes[0,0].tick_params(axis='x', rotation=45)
    for i, v in enumerate(accuracies):
        axes[0,0].text(i, v + 0.005, f'{v:.3f}', ha='center', va='bottom')
    
    # AUCåˆ†æ•°
    aucs = [r['auc'] for r in all_results]
    axes[0,1].bar(models, aucs, color=colors)
    axes[0,1].set_title('AUCåˆ†æ•°å¯¹æ¯”')
    axes[0,1].set_ylabel('AUCåˆ†æ•°')
    axes[0,1].tick_params(axis='x', rotation=45)
    for i, v in enumerate(aucs):
        axes[0,1].text(i, v + 0.005, f'{v:.3f}', ha='center', va='bottom')
    
    # æ¨ç†æ—¶é—´
    inf_times = [r['inference_time'] for r in all_results]
    axes[1,0].bar(models, inf_times, color=colors)
    axes[1,0].set_title('æ¨ç†æ—¶é—´å¯¹æ¯”')
    axes[1,0].set_ylabel('æ¨ç†æ—¶é—´ (ç§’)')
    axes[1,0].tick_params(axis='x', rotation=45)
    for i, v in enumerate(inf_times):
        axes[1,0].text(i, v + v*0.02, f'{v:.2f}', ha='center', va='bottom')
    
    # å‚æ•°æ•°é‡
    params = [r['parameters']/1e6 for r in all_results]  # è½¬æ¢ä¸ºç™¾ä¸‡
    axes[1,1].bar(models, params, color=colors)
    axes[1,1].set_title('æ¨¡å‹å‚æ•°æ•°é‡å¯¹æ¯”')
    axes[1,1].set_ylabel('å‚æ•°æ•°é‡ (ç™¾ä¸‡)')
    axes[1,1].tick_params(axis='x', rotation=45)
    for i, v in enumerate(params):
        axes[1,1].text(i, v + v*0.02, f'{v:.2f}M', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig('results/comprehensive_evaluation/performance_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # 2. ROCæ›²çº¿å¯¹æ¯”
    plt.figure(figsize=(10, 8))
    for i, result in enumerate(all_results):
        plt.plot(result['fpr'], result['tpr'], 
                label=f"{result['model_name']} (AUC = {result['auc']:.3f})",
                color=colors[i], linewidth=2)
    
    plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('å‡æ­£ç‡ (False Positive Rate)')
    plt.ylabel('çœŸæ­£ç‡ (True Positive Rate)')
    plt.title('ROCæ›²çº¿å¯¹æ¯”')
    plt.legend(loc="lower right")
    plt.grid(True, alpha=0.3)
    plt.savefig('results/comprehensive_evaluation/roc_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("âœ… å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ° results/comprehensive_evaluation/ ç›®å½•")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å…¨é¢æ¨¡å‹è¯„ä¼°...")
    print("="*60)
    
    # è®¾å¤‡è®¾ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_loader, X_test, y_test = load_test_data()
    
    # å®šä¹‰è¦è¯„ä¼°çš„æ¨¡å‹
    models_to_evaluate = [
        ('results/comparison/models/cnn1d_epoch_100.pth', 'CNN1D'),
        ('results/comparison/models/lstm_epoch_100.pth', 'LSTM'),
        ('results/comparison/models/resnet1d_epoch_100.pth', 'RESNET1D'),
        ('results/comparison/models/hybrid_cnn_lstm_best.pth', 'HYBRID_CNN_LSTM'),
    
    ]
    
    all_results = []
    
    # è¯„ä¼°æ¯ä¸ªæ¨¡å‹
    for model_path, model_type in models_to_evaluate:
        print(f"\n{'='*40}")
        print(f"è¯„ä¼°æ¨¡å‹: {model_type}")
        print(f"æ¨¡å‹è·¯å¾„: {model_path}")
        
        result = load_and_evaluate_model(model_path, model_type, test_loader, device)
        if result:
            all_results.append(result)
        else:
            print(f"âŒ è·³è¿‡æ¨¡å‹ {model_type}")
    
    if not all_results:
        print("âŒ æ²¡æœ‰æˆåŠŸè¯„ä¼°ä»»ä½•æ¨¡å‹")
        return
    
    # åˆ›å»ºè¯¦ç»†æŠ¥å‘Š
    sorted_results = create_detailed_report(all_results)
    
    # ä¿å­˜ç»“æœ
    save_comprehensive_results(all_results)
    
    # åˆ›å»ºå¯è§†åŒ–
    create_comparison_visualizations(all_results)
    
    print("\nğŸ‰ å…¨é¢æ¨¡å‹è¯„ä¼°å®Œæˆ!")
    print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("  â€¢ results/comprehensive_model_evaluation.csv - CSVæ ¼å¼ç»“æœ")
    print("  â€¢ results/detailed_evaluation_results.pkl - è¯¦ç»†è¯„ä¼°æ•°æ®")
    print("  â€¢ results/comprehensive_evaluation/performance_comparison.png - æ€§èƒ½å¯¹æ¯”å›¾")
    print("  â€¢ results/comprehensive_evaluation/roc_comparison.png - ROCæ›²çº¿å¯¹æ¯”")

if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()